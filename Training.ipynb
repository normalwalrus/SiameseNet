{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class for getting positive and negative classes for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary with (Family) \\\\\\\\ (ID) as the key and the path to the images under that one person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Utils.DatasetClass import SmileDataset\n",
    "import os\n",
    "import random\n",
    "\n",
    "train_file_path = \"./train_relationships.csv\"\n",
    "train_images_path = \"./train/\"\n",
    "\n",
    "all_images = glob(train_images_path + \"*/*/*.jpg\")\n",
    "train_person_to_images = {}\n",
    "val_person_to_images = {}\n",
    "\n",
    "# Getting 0.1 of the total training as validation\n",
    "percentage_val = 0.1\n",
    "train_names = [folder for folder in os.listdir(train_images_path) if  os.path.isdir(os.path.join(train_images_path, folder))]\n",
    "val_families = random.sample(train_names, int(percentage_val * len(train_names)))\n",
    "\n",
    "train_images = []\n",
    "val_images = []\n",
    "\n",
    "for x in all_images:\n",
    "\n",
    "    if x.split(\"\\\\\")[-3] not in val_families:\n",
    "\n",
    "        if x.split(\"\\\\\")[-3] + \"/\" + x.split(\"\\\\\")[-2] not in train_person_to_images:\n",
    "            train_person_to_images[x.split(\"\\\\\")[-3] + \"/\" + x.split(\"\\\\\")[-2]] = [x]\n",
    "\n",
    "        else:\n",
    "            train_person_to_images[x.split(\"\\\\\")[-3] + \"/\" + x.split(\"\\\\\")[-2]].append(x)\n",
    "\n",
    "        train_images.append(x)\n",
    "    \n",
    "    else:\n",
    "        if x.split(\"\\\\\")[-3] + \"/\" + x.split(\"\\\\\")[-2] not in val_person_to_images:\n",
    "            val_person_to_images[x.split(\"\\\\\")[-3] + \"/\" + x.split(\"\\\\\")[-2]] = [x]\n",
    "\n",
    "        else:\n",
    "            val_person_to_images[x.split(\"\\\\\")[-3] + \"/\" + x.split(\"\\\\\")[-2]].append(x)\n",
    "\n",
    "        val_images.append(x)\n",
    "\n",
    "train_people = [x.split(\"\\\\\")[-3] + \"/\" + x.split(\"\\\\\")[-2] for x in train_images]\n",
    "train_people = list(dict.fromkeys(train_people)) # removing the duplicates\n",
    "\n",
    "val_people = [x.split(\"\\\\\")[-3] + \"/\" + x.split(\"\\\\\")[-2] for x in val_images]\n",
    "val_people = list(dict.fromkeys(val_people)) # removing the duplicates\n",
    "\n",
    "relationships = pd.read_csv(train_file_path)\n",
    "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
    "\n",
    "train_relationships = [x for x in relationships if x[0] in train_people and x[1] in train_people] #Check if people are in the training dataset\n",
    "val_relationships = [x for x in relationships if x[0] in val_people and x[1] in val_people]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the SmileDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = SmileDataset(relations = train_relationships, person_to_image= train_person_to_images)\n",
    "trainloader = DataLoader(train_dataset, batch_size= 100, shuffle = True)\n",
    "\n",
    "val_dataset = SmileDataset(relations = val_relationships, person_to_image= val_person_to_images)\n",
    "valloader = DataLoader(val_dataset, batch_size= 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training time!! :) (Work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valloader, val_dataset, device, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    for batch in valloader:\n",
    "        tensor1, tensor2, label = batch\n",
    "        tensor1, tensor2, label = tensor1.to(device), tensor2.to(device), label.float().view(-1,1).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor1, tensor2)\n",
    "            preds = output>0.5\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "        val_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == (label>0.5))\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_acc = running_corrects.item()/len(val_dataset)\n",
    "\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train(model, trainloader, train_dataset, optimizer, device, criterion):\n",
    "    train_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for batch in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tensor1, tensor2, label = batch\n",
    "        tensor1, tensor2, label = tensor1.to(device), tensor2.to(device), label.float().view(-1,1).to(device)\n",
    "        output = model(tensor1, tensor2)\n",
    "\n",
    "        preds = output>0.5\n",
    "        \n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(preds == label)\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_acc = running_corrects.item()/len(train_dataset)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianch\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1], \tval loss: 0.0067308\tacc: 0.55224\n",
      "[1], \ttrain loss: 0.0067433\tacc: 0.56591\n",
      "[2], \tval loss: 0.0064789\tacc: 0.62985\n",
      "[2], \ttrain loss: 0.0062383\tacc: 0.61909\n",
      "[3], \tval loss: 0.0066386\tacc: 0.60149\n",
      "[3], \ttrain loss: 0.0058326\tacc: 0.66386\n",
      "[4], \tval loss: 0.0058546\tacc: 0.66418\n",
      "[4], \ttrain loss: 0.0058292\tacc: 0.66716\n",
      "[5], \tval loss: 0.0067074\tacc: 0.64179\n",
      "[5], \ttrain loss: 0.005601\tacc: 0.68451\n",
      "[6], \tval loss: 0.0063645\tacc: 0.63134\n",
      "[6], \ttrain loss: 0.0055554\tacc: 0.68731\n",
      "[7], \tval loss: 0.0064641\tacc: 0.61343\n",
      "[7], \ttrain loss: 0.0054356\tacc: 0.70152\n",
      "[8], \tval loss: 0.0061657\tacc: 0.66269\n",
      "[8], \ttrain loss: 0.0053273\tacc: 0.71506\n",
      "[9], \tval loss: 0.0065514\tacc: 0.6403\n",
      "[9], \ttrain loss: 0.0053014\tacc: 0.70499\n",
      "[10], \tval loss: 0.0061593\tacc: 0.64328\n",
      "[10], \ttrain loss: 0.0051506\tacc: 0.72762\n",
      "[11], \tval loss: 0.0065439\tacc: 0.6194\n",
      "[11], \ttrain loss: 0.0050983\tacc: 0.72646\n",
      "[12], \tval loss: 0.0065423\tacc: 0.63284\n",
      "[12], \ttrain loss: 0.0051292\tacc: 0.72729\n",
      "[13], \tval loss: 0.006772\tacc: 0.61791\n",
      "[13], \ttrain loss: 0.0050622\tacc: 0.73257\n",
      "[14], \tval loss: 0.0065192\tacc: 0.64776\n",
      "[14], \ttrain loss: 0.00496\tacc: 0.73571\n",
      "[15], \tval loss: 0.0064044\tacc: 0.65672\n",
      "[15], \ttrain loss: 0.0049932\tacc: 0.73142\n",
      "[16], \tval loss: 0.0069247\tacc: 0.64776\n",
      "[16], \ttrain loss: 0.0048405\tacc: 0.74248\n",
      "[17], \tval loss: 0.0064274\tacc: 0.64478\n",
      "[17], \ttrain loss: 0.0048813\tacc: 0.7443\n",
      "[18], \tval loss: 0.0074929\tacc: 0.58358\n",
      "[18], \ttrain loss: 0.0048792\tacc: 0.73588\n",
      "[19], \tval loss: 0.0068092\tacc: 0.64627\n",
      "[19], \ttrain loss: 0.0048025\tacc: 0.74067\n",
      "[20], \tval loss: 0.0069016\tacc: 0.66418\n",
      "[20], \ttrain loss: 0.0048392\tacc: 0.74909\n",
      "[21], \tval loss: 0.007283\tacc: 0.62985\n",
      "[21], \ttrain loss: 0.004716\tacc: 0.7486\n",
      "[22], \tval loss: 0.0061076\tacc: 0.65821\n",
      "[22], \ttrain loss: 0.0047965\tacc: 0.74645\n",
      "[23], \tval loss: 0.0064679\tacc: 0.64627\n",
      "[23], \ttrain loss: 0.0045978\tacc: 0.75619\n",
      "[24], \tval loss: 0.0071575\tacc: 0.62836\n",
      "[24], \ttrain loss: 0.0045615\tacc: 0.76891\n",
      "[25], \tval loss: 0.0068391\tacc: 0.63582\n",
      "[25], \ttrain loss: 0.0045284\tacc: 0.76957\n",
      "[26], \tval loss: 0.0075667\tacc: 0.62687\n",
      "[26], \ttrain loss: 0.004441\tacc: 0.7699\n",
      "[27], \tval loss: 0.0069563\tacc: 0.64776\n",
      "[27], \ttrain loss: 0.0044438\tacc: 0.77172\n",
      "[28], \tval loss: 0.0067393\tacc: 0.68209\n",
      "[28], \ttrain loss: 0.0043648\tacc: 0.77337\n",
      "[29], \tval loss: 0.006713\tacc: 0.65373\n",
      "[29], \ttrain loss: 0.0043715\tacc: 0.77882\n",
      "[30], \tval loss: 0.0071435\tacc: 0.63582\n",
      "[30], \ttrain loss: 0.0044551\tacc: 0.77651\n",
      "[31], \tval loss: 0.0065249\tacc: 0.66567\n",
      "[31], \ttrain loss: 0.0043812\tacc: 0.78229\n",
      "[32], \tval loss: 0.0072265\tacc: 0.63582\n",
      "[32], \ttrain loss: 0.0043625\tacc: 0.77651\n",
      "[33], \tval loss: 0.007341\tacc: 0.6194\n",
      "[33], \ttrain loss: 0.0042765\tacc: 0.78741\n",
      "[34], \tval loss: 0.0067349\tacc: 0.66567\n",
      "[34], \ttrain loss: 0.0042527\tacc: 0.78411\n",
      "[35], \tval loss: 0.0084645\tacc: 0.63881\n",
      "[35], \ttrain loss: 0.0042149\tacc: 0.79237\n",
      "[36], \tval loss: 0.0082627\tacc: 0.63134\n",
      "[36], \ttrain loss: 0.0042221\tacc: 0.79055\n",
      "[37], \tval loss: 0.0071361\tacc: 0.62687\n",
      "[37], \ttrain loss: 0.0043674\tacc: 0.77602\n",
      "[38], \tval loss: 0.0077294\tacc: 0.60896\n",
      "[38], \ttrain loss: 0.0042332\tacc: 0.78758\n",
      "[39], \tval loss: 0.0074841\tacc: 0.62985\n",
      "[39], \ttrain loss: 0.0042284\tacc: 0.78725\n",
      "[40], \tval loss: 0.0077724\tacc: 0.64179\n",
      "[40], \ttrain loss: 0.0040291\tacc: 0.80244\n",
      "[41], \tval loss: 0.007445\tacc: 0.63731\n",
      "[41], \ttrain loss: 0.004217\tacc: 0.78246\n",
      "[42], \tval loss: 0.0073512\tacc: 0.62836\n",
      "[42], \ttrain loss: 0.00409\tacc: 0.80178\n",
      "[43], \tval loss: 0.0080626\tacc: 0.63134\n",
      "[43], \ttrain loss: 0.0040534\tacc: 0.80195\n",
      "[44], \tval loss: 0.0091399\tacc: 0.59254\n",
      "[44], \ttrain loss: 0.0040723\tacc: 0.79204\n",
      "[45], \tval loss: 0.0078201\tacc: 0.65672\n",
      "[45], \ttrain loss: 0.0040215\tacc: 0.79584\n",
      "[46], \tval loss: 0.0075503\tacc: 0.65075\n",
      "[46], \ttrain loss: 0.0040125\tacc: 0.80211\n",
      "[47], \tval loss: 0.0085079\tacc: 0.6209\n",
      "[47], \ttrain loss: 0.0039447\tacc: 0.80856\n",
      "[48], \tval loss: 0.0091803\tacc: 0.61791\n",
      "[48], \ttrain loss: 0.0037628\tacc: 0.81781\n",
      "[49], \tval loss: 0.0081069\tacc: 0.6209\n",
      "[49], \ttrain loss: 0.0040718\tacc: 0.7998\n",
      "[50], \tval loss: 0.0077376\tacc: 0.62388\n",
      "[50], \ttrain loss: 0.0038481\tacc: 0.81252\n",
      "[51], \tval loss: 0.009164\tacc: 0.61642\n",
      "[51], \ttrain loss: 0.0038273\tacc: 0.81236\n",
      "[52], \tval loss: 0.0079765\tacc: 0.62239\n",
      "[52], \ttrain loss: 0.0038273\tacc: 0.81566\n",
      "[53], \tval loss: 0.0084383\tacc: 0.64179\n",
      "[53], \ttrain loss: 0.0038607\tacc: 0.80641\n",
      "[54], \tval loss: 0.0092332\tacc: 0.63582\n",
      "[54], \ttrain loss: 0.0037662\tacc: 0.81186\n",
      "[55], \tval loss: 0.0077169\tacc: 0.6403\n",
      "[55], \ttrain loss: 0.0037253\tacc: 0.81797\n",
      "[56], \tval loss: 0.0084482\tacc: 0.60597\n",
      "[56], \ttrain loss: 0.0038231\tacc: 0.81203\n",
      "[57], \tval loss: 0.0088173\tacc: 0.61194\n",
      "[57], \ttrain loss: 0.0037544\tacc: 0.81814\n",
      "[58], \tval loss: 0.0081732\tacc: 0.6403\n",
      "[58], \ttrain loss: 0.0036325\tacc: 0.82194\n",
      "[59], \tval loss: 0.0086904\tacc: 0.63582\n",
      "[59], \ttrain loss: 0.0038044\tacc: 0.82045\n",
      "[60], \tval loss: 0.0083563\tacc: 0.61791\n",
      "[60], \ttrain loss: 0.0037294\tacc: 0.82061\n",
      "[61], \tval loss: 0.0085448\tacc: 0.64179\n",
      "[61], \ttrain loss: 0.0037001\tacc: 0.82375\n",
      "[62], \tval loss: 0.0076751\tacc: 0.65075\n",
      "[62], \ttrain loss: 0.0035275\tacc: 0.82805\n",
      "[63], \tval loss: 0.0080888\tacc: 0.63582\n",
      "[63], \ttrain loss: 0.0036013\tacc: 0.82574\n",
      "[64], \tval loss: 0.0073115\tacc: 0.66567\n",
      "[64], \ttrain loss: 0.0035771\tacc: 0.82227\n",
      "[65], \tval loss: 0.0093242\tacc: 0.61045\n",
      "[65], \ttrain loss: 0.0036393\tacc: 0.8264\n",
      "[66], \tval loss: 0.0083482\tacc: 0.61791\n",
      "[66], \ttrain loss: 0.0035959\tacc: 0.82524\n",
      "[67], \tval loss: 0.0082668\tacc: 0.64627\n",
      "[67], \ttrain loss: 0.0035683\tacc: 0.83201\n",
      "[68], \tval loss: 0.0090817\tacc: 0.6194\n",
      "[68], \ttrain loss: 0.003498\tacc: 0.83614\n",
      "[69], \tval loss: 0.0096398\tacc: 0.60448\n",
      "[69], \ttrain loss: 0.0033964\tacc: 0.83994\n",
      "[70], \tval loss: 0.0092461\tacc: 0.63433\n",
      "[70], \ttrain loss: 0.0035583\tacc: 0.82904\n",
      "[71], \tval loss: 0.0089745\tacc: 0.63881\n",
      "[71], \ttrain loss: 0.0035183\tacc: 0.83366\n",
      "[72], \tval loss: 0.0079684\tacc: 0.64627\n",
      "[72], \ttrain loss: 0.003555\tacc: 0.83102\n",
      "[73], \tval loss: 0.0081419\tacc: 0.64328\n",
      "[73], \ttrain loss: 0.0034957\tacc: 0.8368\n",
      "[74], \tval loss: 0.010272\tacc: 0.61343\n",
      "[74], \ttrain loss: 0.0034604\tacc: 0.83515\n",
      "[75], \tval loss: 0.0099015\tacc: 0.60299\n",
      "[75], \ttrain loss: 0.0034708\tacc: 0.83565\n",
      "[76], \tval loss: 0.0097919\tacc: 0.62687\n",
      "[76], \ttrain loss: 0.0034548\tacc: 0.83812\n",
      "[77], \tval loss: 0.0095846\tacc: 0.60299\n",
      "[77], \ttrain loss: 0.0035009\tacc: 0.83614\n",
      "[78], \tval loss: 0.010391\tacc: 0.61642\n",
      "[78], \ttrain loss: 0.0032973\tacc: 0.84638\n",
      "[79], \tval loss: 0.0087686\tacc: 0.62836\n",
      "[79], \ttrain loss: 0.0033041\tacc: 0.84556\n",
      "[80], \tval loss: 0.008321\tacc: 0.63582\n",
      "[80], \ttrain loss: 0.0033291\tacc: 0.85249\n",
      "[81], \tval loss: 0.010305\tacc: 0.61493\n",
      "[81], \ttrain loss: 0.0032937\tacc: 0.84027\n",
      "[82], \tval loss: 0.010357\tacc: 0.60597\n",
      "[82], \ttrain loss: 0.0034836\tacc: 0.83267\n",
      "[83], \tval loss: 0.0086024\tacc: 0.62836\n",
      "[83], \ttrain loss: 0.0033902\tacc: 0.83829\n",
      "[84], \tval loss: 0.0087696\tacc: 0.64925\n",
      "[84], \ttrain loss: 0.0033006\tacc: 0.84787\n",
      "[85], \tval loss: 0.0084392\tacc: 0.61791\n",
      "[85], \ttrain loss: 0.0034157\tacc: 0.83267\n",
      "[86], \tval loss: 0.0097883\tacc: 0.62985\n",
      "[86], \ttrain loss: 0.0032714\tacc: 0.8411\n",
      "[87], \tval loss: 0.0092539\tacc: 0.60299\n",
      "[87], \ttrain loss: 0.0032229\tacc: 0.84242\n",
      "[88], \tval loss: 0.0085523\tacc: 0.63433\n",
      "[88], \ttrain loss: 0.0032607\tacc: 0.8477\n",
      "[89], \tval loss: 0.011393\tacc: 0.60746\n",
      "[89], \ttrain loss: 0.0032033\tacc: 0.85613\n",
      "[90], \tval loss: 0.0089639\tacc: 0.63284\n",
      "[90], \ttrain loss: 0.0032292\tacc: 0.8477\n",
      "[91], \tval loss: 0.010512\tacc: 0.62537\n",
      "[91], \ttrain loss: 0.0031456\tacc: 0.85167\n",
      "[92], \tval loss: 0.0097822\tacc: 0.59403\n",
      "[92], \ttrain loss: 0.0031733\tacc: 0.85431\n",
      "[93], \tval loss: 0.0097423\tacc: 0.63731\n",
      "[93], \ttrain loss: 0.0029976\tacc: 0.86026\n",
      "[94], \tval loss: 0.0097279\tacc: 0.62985\n",
      "[94], \ttrain loss: 0.003052\tacc: 0.8662\n",
      "[95], \tval loss: 0.0098918\tacc: 0.62239\n",
      "[95], \ttrain loss: 0.0031924\tacc: 0.85183\n",
      "[96], \tval loss: 0.01036\tacc: 0.61791\n",
      "[96], \ttrain loss: 0.0031653\tacc: 0.85101\n",
      "[97], \tval loss: 0.011659\tacc: 0.59701\n",
      "[97], \ttrain loss: 0.0030761\tacc: 0.85646\n",
      "[98], \tval loss: 0.010773\tacc: 0.62537\n",
      "[98], \ttrain loss: 0.0031106\tacc: 0.85448\n",
      "[99], \tval loss: 0.0092103\tacc: 0.63134\n",
      "[99], \ttrain loss: 0.0031909\tacc: 0.85249\n",
      "[100], \tval loss: 0.010142\tacc: 0.61791\n",
      "[100], \ttrain loss: 0.0031288\tacc: 0.85464\n",
      "[101], \tval loss: 0.009153\tacc: 0.62836\n",
      "[101], \ttrain loss: 0.0032652\tacc: 0.85084\n",
      "[102], \tval loss: 0.011041\tacc: 0.60448\n",
      "[102], \ttrain loss: 0.0031055\tacc: 0.85927\n",
      "[103], \tval loss: 0.0098584\tacc: 0.63433\n",
      "[103], \ttrain loss: 0.0031357\tacc: 0.85183\n",
      "[104], \tval loss: 0.009277\tacc: 0.61493\n",
      "[104], \ttrain loss: 0.0031675\tacc: 0.85728\n",
      "[105], \tval loss: 0.011691\tacc: 0.60597\n",
      "[105], \ttrain loss: 0.0032116\tacc: 0.85233\n",
      "[106], \tval loss: 0.0092611\tacc: 0.65224\n",
      "[106], \ttrain loss: 0.0031212\tacc: 0.85679\n",
      "[107], \tval loss: 0.011139\tacc: 0.60597\n",
      "[107], \ttrain loss: 0.002914\tacc: 0.86951\n",
      "[108], \tval loss: 0.01156\tacc: 0.60597\n",
      "[108], \ttrain loss: 0.0029628\tacc: 0.86059\n",
      "[109], \tval loss: 0.0088245\tacc: 0.63433\n",
      "[109], \ttrain loss: 0.0031113\tacc: 0.852\n",
      "[110], \tval loss: 0.01044\tacc: 0.63284\n",
      "[110], \ttrain loss: 0.0029709\tacc: 0.86373\n",
      "[111], \tval loss: 0.011779\tacc: 0.6209\n",
      "[111], \ttrain loss: 0.0029802\tacc: 0.86521\n",
      "[112], \tval loss: 0.011478\tacc: 0.59701\n",
      "[112], \ttrain loss: 0.002948\tacc: 0.86521\n",
      "[113], \tval loss: 0.0099293\tacc: 0.61493\n",
      "[113], \ttrain loss: 0.0031523\tacc: 0.85547\n",
      "[114], \tval loss: 0.011023\tacc: 0.60597\n",
      "[114], \ttrain loss: 0.0029233\tacc: 0.87166\n",
      "[115], \tval loss: 0.011535\tacc: 0.58657\n",
      "[115], \ttrain loss: 0.0030378\tacc: 0.86439\n",
      "[116], \tval loss: 0.01031\tacc: 0.63284\n",
      "[116], \ttrain loss: 0.0029691\tacc: 0.86439\n",
      "[117], \tval loss: 0.012456\tacc: 0.6\n",
      "[117], \ttrain loss: 0.0029548\tacc: 0.8634\n",
      "[118], \tval loss: 0.01083\tacc: 0.61343\n",
      "[118], \ttrain loss: 0.0027771\tacc: 0.87446\n",
      "[119], \tval loss: 0.010204\tacc: 0.62239\n",
      "[119], \ttrain loss: 0.0029271\tacc: 0.8672\n",
      "[120], \tval loss: 0.010461\tacc: 0.61642\n",
      "[120], \ttrain loss: 0.0028862\tacc: 0.87132\n",
      "[121], \tval loss: 0.011995\tacc: 0.60448\n",
      "[121], \ttrain loss: 0.0028206\tacc: 0.87364\n",
      "[122], \tval loss: 0.010615\tacc: 0.63134\n",
      "[122], \ttrain loss: 0.0029176\tacc: 0.86505\n",
      "[123], \tval loss: 0.011012\tacc: 0.62985\n",
      "[123], \ttrain loss: 0.0028544\tacc: 0.8667\n",
      "[124], \tval loss: 0.010943\tacc: 0.61343\n",
      "[124], \ttrain loss: 0.0028829\tacc: 0.86653\n",
      "[125], \tval loss: 0.0092296\tacc: 0.66119\n",
      "[125], \ttrain loss: 0.002821\tacc: 0.86753\n",
      "[126], \tval loss: 0.011737\tacc: 0.60746\n",
      "[126], \ttrain loss: 0.002662\tacc: 0.88272\n",
      "[127], \tval loss: 0.012356\tacc: 0.61791\n",
      "[127], \ttrain loss: 0.0028615\tacc: 0.87545\n",
      "[128], \tval loss: 0.010853\tacc: 0.6209\n",
      "[128], \ttrain loss: 0.0029302\tacc: 0.86951\n",
      "[129], \tval loss: 0.010881\tacc: 0.63284\n",
      "[129], \ttrain loss: 0.0027961\tacc: 0.87545\n",
      "[130], \tval loss: 0.010581\tacc: 0.60896\n",
      "[130], \ttrain loss: 0.0028224\tacc: 0.87562\n",
      "[131], \tval loss: 0.010525\tacc: 0.61194\n",
      "[131], \ttrain loss: 0.002804\tacc: 0.87793\n",
      "[132], \tval loss: 0.010371\tacc: 0.62687\n",
      "[132], \ttrain loss: 0.0028439\tacc: 0.87265\n",
      "[133], \tval loss: 0.0098353\tacc: 0.61642\n",
      "[133], \ttrain loss: 0.0028627\tacc: 0.87116\n",
      "[134], \tval loss: 0.010807\tacc: 0.62985\n",
      "[134], \ttrain loss: 0.0026984\tacc: 0.8781\n",
      "[135], \tval loss: 0.011753\tacc: 0.6\n",
      "[135], \ttrain loss: 0.0027635\tacc: 0.87463\n",
      "[136], \tval loss: 0.012413\tacc: 0.5806\n",
      "[136], \ttrain loss: 0.0027791\tacc: 0.87248\n",
      "[137], \tval loss: 0.012069\tacc: 0.61642\n",
      "[137], \ttrain loss: 0.0027297\tacc: 0.87347\n",
      "[138], \tval loss: 0.012275\tacc: 0.60896\n",
      "[138], \ttrain loss: 0.0027683\tacc: 0.8738\n",
      "[139], \tval loss: 0.011941\tacc: 0.60299\n",
      "[139], \ttrain loss: 0.0027176\tacc: 0.87578\n",
      "[140], \tval loss: 0.013229\tacc: 0.59403\n",
      "[140], \ttrain loss: 0.0027974\tacc: 0.87578\n",
      "[141], \tval loss: 0.011198\tacc: 0.63433\n",
      "[141], \ttrain loss: 0.0027748\tacc: 0.87364\n",
      "[142], \tval loss: 0.012499\tacc: 0.6\n",
      "[142], \ttrain loss: 0.0027305\tacc: 0.88041\n",
      "[143], \tval loss: 0.011946\tacc: 0.60299\n",
      "[143], \ttrain loss: 0.0027604\tacc: 0.87892\n",
      "[144], \tval loss: 0.010491\tacc: 0.60597\n",
      "[144], \ttrain loss: 0.002789\tacc: 0.87232\n",
      "[145], \tval loss: 0.010479\tacc: 0.59104\n",
      "[145], \ttrain loss: 0.0027578\tacc: 0.87744\n",
      "[146], \tval loss: 0.011831\tacc: 0.6\n",
      "[146], \ttrain loss: 0.0025795\tacc: 0.88553\n",
      "[147], \tval loss: 0.013082\tacc: 0.58955\n",
      "[147], \ttrain loss: 0.002662\tacc: 0.87562\n",
      "[148], \tval loss: 0.0125\tacc: 0.59104\n",
      "[148], \ttrain loss: 0.0027734\tacc: 0.87628\n",
      "[149], \tval loss: 0.012132\tacc: 0.57612\n",
      "[149], \ttrain loss: 0.0025675\tacc: 0.8852\n",
      "[150], \tval loss: 0.011685\tacc: 0.58209\n",
      "[150], \ttrain loss: 0.0026559\tacc: 0.88173\n",
      "[151], \tval loss: 0.012981\tacc: 0.58806\n",
      "[151], \ttrain loss: 0.0026542\tacc: 0.88272\n",
      "[152], \tval loss: 0.013187\tacc: 0.60448\n",
      "[152], \ttrain loss: 0.0026265\tacc: 0.8819\n",
      "[153], \tval loss: 0.010933\tacc: 0.64179\n",
      "[153], \ttrain loss: 0.0026237\tacc: 0.8814\n",
      "[154], \tval loss: 0.013428\tacc: 0.61642\n",
      "[154], \ttrain loss: 0.0026181\tacc: 0.88223\n",
      "[155], \tval loss: 0.013083\tacc: 0.6209\n",
      "[155], \ttrain loss: 0.0025583\tacc: 0.88553\n",
      "[156], \tval loss: 0.012458\tacc: 0.6\n",
      "[156], \ttrain loss: 0.0026349\tacc: 0.8819\n",
      "[157], \tval loss: 0.012439\tacc: 0.58806\n",
      "[157], \ttrain loss: 0.0026108\tacc: 0.88652\n",
      "[158], \tval loss: 0.011933\tacc: 0.60597\n",
      "[158], \ttrain loss: 0.0027541\tacc: 0.88008\n",
      "[159], \tval loss: 0.012494\tacc: 0.61045\n",
      "[159], \ttrain loss: 0.002554\tacc: 0.88768\n",
      "[160], \tval loss: 0.013108\tacc: 0.56418\n",
      "[160], \ttrain loss: 0.0026387\tacc: 0.87925\n",
      "[161], \tval loss: 0.013238\tacc: 0.59403\n",
      "[161], \ttrain loss: 0.0024893\tacc: 0.89049\n",
      "[162], \tval loss: 0.014644\tacc: 0.59552\n",
      "[162], \ttrain loss: 0.0026199\tacc: 0.88503\n",
      "[163], \tval loss: 0.011837\tacc: 0.6\n",
      "[163], \ttrain loss: 0.0026296\tacc: 0.88669\n",
      "[164], \tval loss: 0.012456\tacc: 0.58358\n",
      "[164], \ttrain loss: 0.0025656\tacc: 0.88867\n",
      "[165], \tval loss: 0.01204\tacc: 0.60746\n",
      "[165], \ttrain loss: 0.0026582\tacc: 0.88074\n",
      "[166], \tval loss: 0.012356\tacc: 0.60149\n",
      "[166], \ttrain loss: 0.0024599\tacc: 0.88652\n",
      "[167], \tval loss: 0.011198\tacc: 0.59552\n",
      "[167], \ttrain loss: 0.0026345\tacc: 0.88305\n",
      "[168], \tval loss: 0.01319\tacc: 0.61343\n",
      "[168], \ttrain loss: 0.0025652\tacc: 0.88537\n",
      "[169], \tval loss: 0.011573\tacc: 0.61194\n",
      "[169], \ttrain loss: 0.00261\tacc: 0.88487\n",
      "[170], \tval loss: 0.013363\tacc: 0.59104\n",
      "[170], \ttrain loss: 0.0026133\tacc: 0.88619\n",
      "[171], \tval loss: 0.012988\tacc: 0.61194\n",
      "[171], \ttrain loss: 0.0024506\tacc: 0.89511\n",
      "[172], \tval loss: 0.013876\tacc: 0.61194\n",
      "[172], \ttrain loss: 0.0025474\tacc: 0.88404\n",
      "[173], \tval loss: 0.01203\tacc: 0.61791\n",
      "[173], \ttrain loss: 0.0025117\tacc: 0.88718\n",
      "[174], \tval loss: 0.013744\tacc: 0.58209\n",
      "[174], \ttrain loss: 0.0025272\tacc: 0.89313\n",
      "[175], \tval loss: 0.015306\tacc: 0.59851\n",
      "[175], \ttrain loss: 0.0023895\tacc: 0.89478\n",
      "[176], \tval loss: 0.012442\tacc: 0.58955\n",
      "[176], \ttrain loss: 0.0024527\tacc: 0.89049\n",
      "[177], \tval loss: 0.01282\tacc: 0.59851\n",
      "[177], \ttrain loss: 0.0024913\tacc: 0.88718\n",
      "[178], \tval loss: 0.013993\tacc: 0.61343\n",
      "[178], \ttrain loss: 0.0025455\tacc: 0.88603\n",
      "[179], \tval loss: 0.013015\tacc: 0.61791\n",
      "[179], \ttrain loss: 0.0025485\tacc: 0.88586\n",
      "[180], \tval loss: 0.012349\tacc: 0.61791\n",
      "[180], \ttrain loss: 0.0024179\tacc: 0.89792\n",
      "[181], \tval loss: 0.013873\tacc: 0.60896\n",
      "[181], \ttrain loss: 0.0024065\tacc: 0.89676\n",
      "[182], \tval loss: 0.012828\tacc: 0.60299\n",
      "[182], \ttrain loss: 0.0024435\tacc: 0.89577\n",
      "[183], \tval loss: 0.013491\tacc: 0.59851\n",
      "[183], \ttrain loss: 0.002413\tacc: 0.90023\n",
      "[184], \tval loss: 0.013927\tacc: 0.58806\n",
      "[184], \ttrain loss: 0.0026076\tacc: 0.88751\n",
      "[185], \tval loss: 0.012287\tacc: 0.61493\n",
      "[185], \ttrain loss: 0.0024087\tacc: 0.89495\n",
      "[186], \tval loss: 0.014847\tacc: 0.58955\n",
      "[186], \ttrain loss: 0.0023898\tacc: 0.89313\n",
      "[187], \tval loss: 0.011922\tacc: 0.63731\n",
      "[187], \ttrain loss: 0.002417\tacc: 0.89445\n",
      "[188], \tval loss: 0.012243\tacc: 0.59552\n",
      "[188], \ttrain loss: 0.0024507\tacc: 0.89462\n",
      "[189], \tval loss: 0.011565\tacc: 0.63284\n",
      "[189], \ttrain loss: 0.0025032\tacc: 0.89329\n",
      "[190], \tval loss: 0.01368\tacc: 0.63582\n",
      "[190], \ttrain loss: 0.0023758\tacc: 0.89577\n",
      "[191], \tval loss: 0.014034\tacc: 0.5806\n",
      "[191], \ttrain loss: 0.0024673\tacc: 0.8923\n",
      "[192], \tval loss: 0.011936\tacc: 0.60896\n",
      "[192], \ttrain loss: 0.0023934\tacc: 0.89495\n",
      "[193], \tval loss: 0.013845\tacc: 0.60896\n",
      "[193], \ttrain loss: 0.0023213\tacc: 0.9004\n",
      "[194], \tval loss: 0.013386\tacc: 0.57612\n",
      "[194], \ttrain loss: 0.0022957\tacc: 0.90139\n",
      "[195], \tval loss: 0.013188\tacc: 0.58657\n",
      "[195], \ttrain loss: 0.002473\tacc: 0.8923\n",
      "[196], \tval loss: 0.013809\tacc: 0.57015\n",
      "[196], \ttrain loss: 0.002512\tacc: 0.89346\n",
      "[197], \tval loss: 0.013453\tacc: 0.59701\n",
      "[197], \ttrain loss: 0.0024902\tacc: 0.89098\n",
      "[198], \tval loss: 0.013328\tacc: 0.58507\n",
      "[198], \ttrain loss: 0.0023903\tacc: 0.89511\n",
      "[199], \tval loss: 0.014081\tacc: 0.59254\n",
      "[199], \ttrain loss: 0.0023717\tacc: 0.89808\n",
      "[200], \tval loss: 0.012212\tacc: 0.60896\n",
      "[200], \ttrain loss: 0.0023984\tacc: 0.89941\n",
      "[201], \tval loss: 0.013355\tacc: 0.59552\n",
      "[201], \ttrain loss: 0.0022885\tacc: 0.89775\n",
      "[202], \tval loss: 0.011598\tacc: 0.63284\n",
      "[202], \ttrain loss: 0.0023208\tacc: 0.89742\n",
      "[203], \tval loss: 0.013068\tacc: 0.59851\n",
      "[203], \ttrain loss: 0.0024214\tacc: 0.89676\n",
      "[204], \tval loss: 0.015032\tacc: 0.56716\n",
      "[204], \ttrain loss: 0.0023141\tacc: 0.8966\n",
      "[205], \tval loss: 0.013005\tacc: 0.61194\n",
      "[205], \ttrain loss: 0.0023346\tacc: 0.89676\n",
      "[206], \tval loss: 0.017241\tacc: 0.58507\n",
      "[206], \ttrain loss: 0.0022662\tacc: 0.90205\n",
      "[207], \tval loss: 0.013378\tacc: 0.57612\n",
      "[207], \ttrain loss: 0.0024409\tacc: 0.8923\n",
      "[208], \tval loss: 0.015863\tacc: 0.57313\n",
      "[208], \ttrain loss: 0.0022601\tacc: 0.90287\n",
      "[209], \tval loss: 0.015525\tacc: 0.5791\n",
      "[209], \ttrain loss: 0.0023419\tacc: 0.89428\n",
      "[210], \tval loss: 0.016464\tacc: 0.57612\n",
      "[210], \ttrain loss: 0.0023107\tacc: 0.90188\n",
      "[211], \tval loss: 0.016229\tacc: 0.56567\n",
      "[211], \ttrain loss: 0.002367\tacc: 0.89874\n",
      "[212], \tval loss: 0.015745\tacc: 0.58657\n",
      "[212], \ttrain loss: 0.0022176\tacc: 0.90568\n",
      "[213], \tval loss: 0.0144\tacc: 0.58507\n",
      "[213], \ttrain loss: 0.0023389\tacc: 0.90139\n",
      "[214], \tval loss: 0.014246\tacc: 0.60149\n",
      "[214], \ttrain loss: 0.0021456\tacc: 0.90023\n",
      "[215], \tval loss: 0.013013\tacc: 0.58657\n",
      "[215], \ttrain loss: 0.0023666\tacc: 0.89907\n",
      "[216], \tval loss: 0.0145\tacc: 0.59403\n",
      "[216], \ttrain loss: 0.0024179\tacc: 0.89329\n",
      "[217], \tval loss: 0.013394\tacc: 0.6194\n",
      "[217], \ttrain loss: 0.0023003\tacc: 0.90436\n",
      "[218], \tval loss: 0.013908\tacc: 0.57164\n",
      "[218], \ttrain loss: 0.0023485\tacc: 0.89792\n",
      "[219], \tval loss: 0.013908\tacc: 0.60597\n",
      "[219], \ttrain loss: 0.0022866\tacc: 0.90552\n",
      "[220], \tval loss: 0.015818\tacc: 0.5806\n",
      "[220], \ttrain loss: 0.0022414\tacc: 0.90353\n",
      "[221], \tval loss: 0.014881\tacc: 0.57463\n",
      "[221], \ttrain loss: 0.0022451\tacc: 0.90486\n",
      "[222], \tval loss: 0.013418\tacc: 0.60746\n",
      "[222], \ttrain loss: 0.002305\tacc: 0.9004\n",
      "[223], \tval loss: 0.014942\tacc: 0.60149\n",
      "[223], \ttrain loss: 0.00216\tacc: 0.90766\n",
      "[224], \tval loss: 0.016949\tacc: 0.56716\n",
      "[224], \ttrain loss: 0.0022096\tacc: 0.90469\n",
      "[225], \tval loss: 0.015146\tacc: 0.56716\n",
      "[225], \ttrain loss: 0.0022169\tacc: 0.9075\n",
      "[226], \tval loss: 0.015561\tacc: 0.58358\n",
      "[226], \ttrain loss: 0.0023037\tacc: 0.90238\n",
      "[227], \tval loss: 0.016378\tacc: 0.57313\n",
      "[227], \ttrain loss: 0.0022394\tacc: 0.9075\n",
      "[228], \tval loss: 0.013717\tacc: 0.59254\n",
      "[228], \ttrain loss: 0.0023472\tacc: 0.89544\n",
      "[229], \tval loss: 0.015733\tacc: 0.57463\n",
      "[229], \ttrain loss: 0.0021743\tacc: 0.90535\n",
      "[230], \tval loss: 0.012778\tacc: 0.61194\n",
      "[230], \ttrain loss: 0.0020804\tacc: 0.91196\n",
      "[231], \tval loss: 0.014878\tacc: 0.58657\n",
      "[231], \ttrain loss: 0.0022739\tacc: 0.90271\n",
      "[232], \tval loss: 0.013064\tacc: 0.5597\n",
      "[232], \ttrain loss: 0.0023398\tacc: 0.90188\n",
      "[233], \tval loss: 0.014382\tacc: 0.56567\n",
      "[233], \ttrain loss: 0.0020948\tacc: 0.91196\n",
      "[234], \tval loss: 0.014591\tacc: 0.58209\n",
      "[234], \ttrain loss: 0.0021569\tacc: 0.91031\n",
      "[235], \tval loss: 0.019318\tacc: 0.57463\n",
      "[235], \ttrain loss: 0.002203\tacc: 0.90337\n",
      "[236], \tval loss: 0.012465\tacc: 0.61791\n",
      "[236], \ttrain loss: 0.0021934\tacc: 0.9037\n",
      "[237], \tval loss: 0.013863\tacc: 0.6\n",
      "[237], \ttrain loss: 0.0021621\tacc: 0.90899\n",
      "[238], \tval loss: 0.014863\tacc: 0.59254\n",
      "[238], \ttrain loss: 0.0022839\tacc: 0.90469\n",
      "[239], \tval loss: 0.015674\tacc: 0.58955\n",
      "[239], \ttrain loss: 0.0022088\tacc: 0.90816\n",
      "[240], \tval loss: 0.014332\tacc: 0.58955\n",
      "[240], \ttrain loss: 0.0022046\tacc: 0.90238\n",
      "[241], \tval loss: 0.016497\tacc: 0.58657\n",
      "[241], \ttrain loss: 0.002165\tacc: 0.907\n",
      "[242], \tval loss: 0.013224\tacc: 0.61194\n",
      "[242], \ttrain loss: 0.0022235\tacc: 0.90899\n",
      "[243], \tval loss: 0.012884\tacc: 0.60896\n",
      "[243], \ttrain loss: 0.0021804\tacc: 0.90601\n",
      "[244], \tval loss: 0.015394\tacc: 0.59254\n",
      "[244], \ttrain loss: 0.0021538\tacc: 0.90915\n",
      "[245], \tval loss: 0.01765\tacc: 0.58358\n",
      "[245], \ttrain loss: 0.0020996\tacc: 0.91014\n",
      "[246], \tval loss: 0.016745\tacc: 0.6\n",
      "[246], \ttrain loss: 0.0022351\tacc: 0.90453\n",
      "[247], \tval loss: 0.014899\tacc: 0.60149\n",
      "[247], \ttrain loss: 0.0021234\tacc: 0.90849\n",
      "[248], \tval loss: 0.013251\tacc: 0.61045\n",
      "[248], \ttrain loss: 0.0020945\tacc: 0.91328\n",
      "[249], \tval loss: 0.013495\tacc: 0.60149\n",
      "[249], \ttrain loss: 0.0021303\tacc: 0.91179\n",
      "[250], \tval loss: 0.014247\tacc: 0.59254\n",
      "[250], \ttrain loss: 0.0021354\tacc: 0.90965\n"
     ]
    }
   ],
   "source": [
    "from Utils.SiameseNet import SiameseNet, SiameseNet_large\n",
    "import os\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = SiameseNet_large().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params= model.parameters(), lr = lr)\n",
    "\n",
    "num_epoch = 250\n",
    "best_epoch = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "history = []\n",
    "accuracy = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    train_loss, train_acc = train(model, trainloader, train_dataset, optimizer, device, criterion)\n",
    "    val_loss, val_acc  = validate(model, valloader, val_dataset, device, criterion)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "\n",
    "        save_path = os.getcwd() +'\\\\models\\\\SiameseNetLarge_best.pt'\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "    print('[{}], \\tval loss: {:.5}\\tacc: {:.5}'.format(epoch+1, val_loss, val_acc))\n",
    "    print('[{}], \\ttrain loss: {:.5}\\tacc: {:.5}'.format(epoch+1, train_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best epoch : 27\n"
     ]
    }
   ],
   "source": [
    "print('best epoch : {}'.format(best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.getcwd() +'\\\\models\\\\SiameseNetLarge_epoch{}.pt'.format(epoch)\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
